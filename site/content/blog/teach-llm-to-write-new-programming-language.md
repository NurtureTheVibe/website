+++
title = "Let's Teach An LLM To Write A New Programming Language"
date = 2025-06-09T19:03:00
weight = 2
template = "post.html"
[taxonomies]
tags = [
  "llms",
  "vibe-coding",
  "prompt-engineering",
  "programming-languages",
  "open-source",
  "ai-assisted-dev",
  "raku"
]
[extra]
hero_image = "/images/data-mining.png"
summary = "How can we teach an LLM to write a new programming language?"
+++
## Introduction

In a [previous blog post](/blog/the-end-of-new-programming-languages/), we discussed how LLMs often struggle with more niche programming languages, and how this is likely to be an issue for languages looking to gain adoption in an era where a lot of new code is generated by LLMs. If you haven't read that blog post yet, I'd suggest checking it out before reading this one.

In summary, we suggested a strategy for producing datasets specifically for training LLMs. In this blog post, we're going to talk about how to scrape, format and manage these digital treasure troves. 

Since I have not, as of today, created a programming language, I'll be teaching an LLM to write [Raku](https://raku.org) (formerly known as Perl 6). Most LLMs have little to no capability to write Raku, so this is a perfect language to use as an example.

This blog post isn't trying to detail the optimal method for data gathering, which would require thousands of hours of annotation and synthetic data generation. Instead, we're looking to collate enough data to successfully teach an LLM to be better at Raku than it was before.

## Base &amp; Instruct Models

There are two generally available types of open weights LLM models:

- **Instruct models** are the type that you're used to using. They are designed to take instructions from the user, and produce responses that are accurate and relevant. For example 'How do I solve the Tower of Hanoi with Raku?' should produce a response that answers the question and includes the Raku code to solve the Tower of Hanoi.
- **Base models** are the precursor to instruct models. They are trained on a large corpus of unstructured data, but they don't know how to produce useful responses from it, they function essentially as autocomplete.

To train a model on a niche language, we will start with a base model, introduce **domain-specific information** (in this case, Raku), and then fine-tune it further to understand how to turn that 'knowledge' into useful conversational responses.

In summary:

- **Continued Pretraining**: Teach the base model about Rakuâ€™s syntax, ecosystem, and available libraries.
- **Supervised Fine-tuning**: Teach the model to answer Raku-related questions in a conversational manner.

## Data Sources

### Continued Pretraining

The data for CPT is typically in `jsonl` (JSON Lines) format, and usually looks something like this:

```jsonl
{"txt": "Paris is the Capitol city of France..."}
{"txt": "Moscow is the Capitol city of Russia..."}
...
```

So, what sources can we use for unstructured data about Raku? The ones I've selected for the proof of concept are:

- The **official Raku documentation** (Artistic 2.0 License): Rich with code examples and comprehensive details about the language's features.
- **Raku Land** README files (various licenses): A listing of ZEF, the language's main open source package repository, with plenty of usage examples.
- **GitHub Sources** (various licenses): The source code for thousands of open source Raku packages, providing a broad corpus of various usages of the language.

If you wanted to expand on this, there's a lot that you could add:

- Blog posts
- Books
- Peer-reviewed papers
- Newsgroups

Ideally, every function would be explicitly annotated, but due to time constraints, we'll rely primarily on existing comments.

On top of this, LLMs are subject to something called [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference), whereby the model forgets things that it already knew upon encountering new data. In order to combat this, we're going to pair our Raku corpus with a subset of a broader open corpus called [RedPajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample).

Our CPT split will be approximately:

- 40% Raku-specific corpus
- 60% broad corpus

This will give us our new, Raku-aware, base model.

### Instruct Training

The data for instruct training is also often in a `jsonl` format, but this time it's structured in a conversational format called ShareGPT:

```jsonl
{"messages":[{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi there!"}]}
{"messages":[{"role": "user", "content": "What is the capitol of the England?"}, {"role": "assistant", "content": "The Capitol City of England is London."}]}
...
```

The sources that we can use for conversational data are numerous, but some are simpler to keep well-formatted than others. We're going to use:

- **StackOverflow question/answer pairs** (CC BY-SA Licensed): We will search for questions tagged `raku` with accepted answers, then store the question as the `user` message and the accepted answer as the `assistant` message.
- **RosettaCode** examples (GFDL Licensed): The puzzle will be the `user` message, and the solution will be the `assistant` message.

This will give us a few thousand conversations, though they will all be single turn (user and assistant response) rather than conversational.

To do this in a more ideal way, where you're looking to control the tone, verbosity and vocabulary of the outputs, you would ideally curate the question &amp; answer pairs, and get multi-turn conversational data in there as well.

Some good sources you could add to be more thorough:

- Pull requests (though these would need a lot of manual data transformation so that the patch becomes assistant's `solution`)
- Discord/IRC channels
- Mailing lists

Another way you could approach this is by making synthetic conversational data, where you inject relevant docs from your CPT data into a prompt with RAG or lorebooks and then remove them from the conversational data. This is outside the scope of this blog post, though.

Again, to avoid catastrophic forgetting as well as to add multi-turn conversational data, we're going to mix in a few more public datasets at various weightings:

- ~30% Raku-specific question &amp; answer pairs
- ~30% other programming language conversations
- ~40% broader corpus multi-turn conversations

Once this training stage is applied to our base model, we will have an instruct model which will hopefully be able to code some Raku with us.

## Gathering &amp; Processing the Data

Our Raku data will be gathered using web scraping scripts. They're easy to prompt for [using the RAIN method with ChatGPT](/blog/write-better-dev-prompts-using-rain-method) or your LLM of choice.

For the CPT data, you're going to want everything in Markdown files or code sources. Telling the LLM to "use `beautifulsoup` and `markdownify`" will help to keep the scripts consistent. Make sure it sets a delay between requests too with a graceful backoff mechanism. The goal of this task is not to speedrun getting banned from GitHub or Raku Land.

For the conversational data, you're going to need to tell the LLM what will be used as the `user` message and what will be used as the `assistant` message. It's often easy to go straight to the `sharegpt` format (JSONL example above) here, and avoid any intermediate steps.

I ended up building four scripts with ChatGPT, which took about half an hour in total:

### StackOverflow

The first script scrapes StackOverflow for questions tagged `raku` with an accepted answer. For each question, it fetches the original question text and sticks it in the `user` message, then fetches the accepted answer in the `assistant` message. All of the question &amp; answer pairs are stored in one file called `train/stackoverflow.jsonl`.

### Rosetta Code

The second script scrapes RosettaCode, starting at the Raku category page and working through the list of problems solved in Raku. It fetches the problem description for each and puts it in the `user` message, then fetches everything under the `Raku` header in the solutions and puts it in the `assistant` message. Everything is stored in one file called `train/rosettacode.jsonl`.

### Raku Docs

The third script scrapes the official docs pages from [raku.org](https://rakudo.org) and parses each one into a valid Markdown file which is stored in `raw/{page_slug}.md`.

### Raku Land

The fourth script iterates through every package listed on [Raku Land](https://raku.land), then for each project it:

- Clones the `README` in Markdown format to `raw/dist_{package_name}.md`
- Clones the linked GitHub repository to `dists/{project_name}`

## Where We Are &amp; Next Steps

Our instruct data is already prepared for training, but our continued pretraining data needs a few more steps before we move onto actually training a model:

- Strip irrelevant files from the source code
- Decide what model we're going to train and the sequence length we plan to train at
- Pack the data correctly for training (splitting on sequence length, with some overlap)

In the next blog post, we'll look at how this data processing works, and how to configure a training run.

## A Note About Ethics &amp; Copyright

AI training raises complex ethical and legal issues. Opinions vary significantly about the legality and ethics of large-scale data collection.

To limit any real-world impact, we will:

- Use publicly available data with clear open licenses
- Develop an LLM with very modest capabilities to minimise the risk to the developer jobs market
- Preserve attribution and licensing information for the original authors of the training data
- Publish the resulting model &amp; datasets on Huggingface under open licences

This won't satisfy every reader, but it reflects a commitment to navigating a complex ethical landscape responsibly.
